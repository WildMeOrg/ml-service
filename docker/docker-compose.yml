version: '3.8'

services:
  detector_inference:
    build:
      context: ..
      dockerfile: docker/dockerfile
    container_name: detector_inference
    ports:
      - "127.0.0.1:6050:6050"
    volumes:
      - ../app:/app/app
      - ../requirements.txt:/app/requirements.txt
      - ../app/model_config.json:/app/model_config.json:ro
      - ultralytics_config:/root/.config/Ultralytics
      - "$DATASETS_DIR:/datasets"
      # to share paths with WBIA:
      - "$DATA_DB_DIR:/data/db"
    environment:
      - PYTHONPATH=/app
    command: python3 -m app.main --host 0.0.0.0 --port 6050 --reload --workers 4 --device cuda
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]
          memory: '32G'
    shm_size: '16G'
    restart: unless-stopped
    labels:
      - "autoheal=true"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6050/health"]
      interval: 30s
      timeout: 15s
      retries: 3
      start_period: 90s
    networks:
      - shared_net

networks:
  shared_net:
    external: true

volumes:
  ultralytics_config:
